{
    "abstract": "After building a classifier with modern tools of machine learning we\ntypically have a black box at hand that is able to predict well for\nunseen data. Thus, we get an answer to the question <i>what</i> is\nthe most likely label of a given unseen data point.  However, most methods will provide no answer <i>why</i> the model predicted a particular label for a single instance and what features were most influential for that particular instance.  The only method that is currently able to provide such explanations are decision trees.  This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of <i>any</i> classification method.",
    "authors": [
        "David Baehrens",
        "Timon Schroeter",
        "Stefan Harmeling",
        "Motoaki Kawanabe",
        "Katja Hansen",
        "Klaus-Robert M{{\\\"u}}ller"
    ],
    "id": "baehrens10a",
    "issue": 61,
    "pages": [
        1803,
        1831
    ],
    "title": "How to Explain Individual Classification Decisions",
    "volume": "11",
    "year": "2010"
}