{
    "abstract": "Model selection strategies for machine learning algorithms typically involve\nthe numerical optimisation of an appropriate model selection criterion, often\nbased on an estimator of generalisation performance, such as <i>k</i>-fold \ncross-validation.  The error of such an estimator can be broken down into bias \nand variance components.  While unbiasedness is often cited as a beneficial \nquality of a model selection criterion, we demonstrate that a low variance is \nat least as important, as a non-negligible variance introduces the potential \nfor over-fitting in model selection as well as in training the model.  While \nthis observation is in hindsight perhaps rather obvious, the degradation in \nperformance due to over-fitting the model selection criterion can be \nsurprisingly large, an observation that appears to have received little \nattention in the machine learning literature to date.  In this paper, we show \nthat the effects of this form of over-fitting are often of comparable \nmagnitude to differences in performance between learning algorithms, and thus \ncannot be ignored in empirical evaluation.  Furthermore, we show that some \ncommon performance evaluation practices are susceptible to a form of selection \nbias as a result of this form of over-fitting and hence are unreliable.  We \ndiscuss methods to avoid over-fitting in model selection and subsequent \nselection bias in performance evaluation, which we hope will be incorporated \ninto best practice.  While this study concentrates on cross-validation based \nmodel selection, the findings are quite general and apply to any model \nselection practice involving the optimisation of a model selection criterion \nevaluated over a finite sample of data, including maximisation of the Bayesian \nevidence and optimisation of performance bounds.",
    "authors": [
        "Gavin C. Cawley",
        "Nicola L. C. Talbot"
    ],
    "id": "cawley10a",
    "issue": 70,
    "pages": [
        2079,
        2107
    ],
    "title": "On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation",
    "volume": "11",
    "year": "2010"
}